import warningswarnings.filterwarnings('ignore')from glob import globimport pandas as pdimport numpy as npfrom tqdm import tqdmimport cv2import osimport timmimport randomimport torchfrom torch.utils.data import Dataset, DataLoaderimport torch.nn as nnimport torchvision.transforms as transformsfrom sklearn.metrics import f1_score, accuracy_scoreimport timedevice = torch.device('cuda')train_png = sorted(glob('train/*/*.png'))test_png = sorted(glob('test/*.png'))train_y = pd.read_csv('train_df.csv')train_labels = train_y['label']label_unique = sorted(np.unique(train_labels))label_unique = {key: value for key, value in zip(label_unique, range(len(label_unique)))}train_labels = [label_unique[k] for k in train_labels]len(train_labels)def img_load(path):    img = cv2.imread(path)[:, :, ::-1]    img = cv2.resize(img, (512, 512))    return imgtrain_imgs = [img_load(m) for m in tqdm(train_png)]test_imgs = [img_load(n) for n in tqdm(test_png)]class Custom_dataset(Dataset):    def __init__(self, img_paths, labels, mode='train'):        self.img_paths = img_paths        self.labels = labels        self.mode = mode    def __len__(self):        return len(self.img_paths)    def __getitem__(self, idx):        img = self.img_paths[idx]        if self.mode == 'train':            augmentation = random.randint(0, 2)            if augmentation == 1:                img = img[::-1].copy()            elif augmentation == 2:                img = img[:, ::-1].copy()        img = transforms.ToTensor()(img)        if self.mode == 'test':            pass        label = self.labels[idx]        return img, labelclass Network(nn.Module):    def __init__(self):        super(Network, self).__init__()        self.model = timm.create_model('resnetrs420', pretrained=True, drop_rate=0.3,                                       checkpoint_path='./pytorch_resnet_rs/model/resnetrs420.pt')    def forward(self, x):        x = self.model(x)        num_features = x.fc.in_features        x.fc = nn.Linear(num_features, 88)        return xbatch_size = 32epochs = 25# Traintrain_dataset = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train')train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=4)# Testtest_dataset = Custom_dataset(np.array(test_imgs), np.array(['tmp'] * len(test_imgs)), mode='test')test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)def score_function(real, pred):    score = f1_score(real, pred, average='macro')    return scoremodel = Network().to(device)optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)criterion = nn.CrossEntropyLoss()scaler = torch.cuda.amp.GradScaler()best = 0for epoch in range(epochs):    start = time.time()    train_loss = 0    train_pred = []    train_y = []    model.train()    for batch in (train_loader):        optimizer.zero_grad()        x = torch.tensor(batch[0], dtype=torch.float32, device=device)        y = torch.tensor(batch[1], dtype=torch.long, device=device)        with torch.cuda.amp.autocast():            pred = model(x)        loss = criterion(pred, y)        scaler.scale(loss).backward()        scaler.step(optimizer)        scaler.update()        train_loss += loss.item() / len(train_loader)        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()        train_y += y.detach().cpu().numpy().tolist()    train_f1 = score_function(train_y, train_pred)    TIME = time.time() - start    print(f'epoch : {epoch + 1}/{epochs}    time : {TIME:.0f}s/{TIME * (epochs - epoch - 1):.0f}s')    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')model.eval()f_pred = []with torch.no_grad():    for batch in (test_loader):        x = torch.tensor(batch[0], dtype=torch.float32, device=device)        with torch.cuda.amp.autocast():            pred = model(x)        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())label_decoder = {val: key for key, val in label_unique.items()}f_result = [label_decoder[result] for result in f_pred]submission = pd.read_csv('sample_submission.csv')submission['label'] = f_resultsubmission.to_csv('baseline.csv', index=False)